{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "\n",
    "import time\n",
    "\n",
    "# Core of scraping\n",
    "import requests\n",
    "from os.path import join as pjoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# For applying a random sleep interval between requests  \n",
    "from random import randint \n",
    "from time import sleep\n",
    "\n",
    "# Need these in order to simulate human activity in Chrome/Firefox browser (clicking)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "\n",
    "# Regex\n",
    "import regex as re\n",
    "import os \n",
    "import sys\n",
    "\n",
    "# Zip\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Needed for turning the date to datetime values\n",
    "import datetime as dt\n",
    "\n",
    "# Dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Twilio enables us to send SMS\n",
    "from twilio.rest import Client\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "# URLS we need these specific URLs in order to run te program correctly. Do not change this !\n",
    "inside_airbnb_url = \"http://insideairbnb.com/\"\n",
    "get_the_data_url = \"http://insideairbnb.com/get-the-data.html\"\n",
    "\n",
    "################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     2,
     25,
     53,
     68,
     76,
     100,
     110,
     182,
     192,
     201,
     227,
     233,
     243,
     263,
     285,
     301,
     317,
     325,
     334,
     346,
     369,
     400,
     459,
     466,
     483,
     505
    ]
   },
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "\n",
    "def try_connecting(get_the_data_url):\n",
    "    while True:\n",
    "        try:\n",
    "            source_code = requests.get(get_the_data_url, timeout = 30, verify=False)\n",
    "            return (source_code)\n",
    "        except (requests.ConnectionError) as e:\n",
    "            print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "            print(str(e))            \n",
    "            continue\n",
    "        except (requests.Timeout) as e:\n",
    "            print(\"OOPS!! Timeout Error\")\n",
    "            print(str(e))\n",
    "            continue\n",
    "        except (requests.RequestException) as e:\n",
    "            print(\"OOPS!! General Error\")\n",
    "            print(str(e))\n",
    "            continue\n",
    "        except (KeyboardInterrupt):\n",
    "            print(\"Someone closed the program\")\n",
    "        break\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def make_cities_list(soup):\n",
    "    ''''''\n",
    "    cities_list = [] # List to store the list of cities\n",
    "\n",
    "    for location in soup.findAll(re.compile('^h2$')):\n",
    "        l = location.text\n",
    "        #print(l)\n",
    "        s = l.split(\",\")\n",
    "        #print(s)\n",
    "        n = []\n",
    "        \n",
    "        for i in s:\n",
    "            if i[0] == \" \":\n",
    "                k = i[1:]\n",
    "            else:\n",
    "                k = i\n",
    "            n.append(k)\n",
    "        # City, county and country included here \n",
    "        cities_list.append(n)\n",
    "    \n",
    "    list_of_cities = []\n",
    "    for c in cities_list:\n",
    "        list_of_cities.append(c[0])\n",
    "\n",
    "    return (list_of_cities)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def make_cities_index_list(cities_list):\n",
    "    ''''''\n",
    "    # Only names of cities\n",
    "    list_of_cities_index = []\n",
    "    index = 0 # We need the index to find the table, because I can't retrieve it by id\n",
    "\n",
    "    # We'll build a list of tuples that stores (city,index)\n",
    "    for c in cities_list:\n",
    "        list_of_cities_index.append((index, c))\n",
    "        index += 1 \n",
    "    return list_of_cities_index\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def link_core(link):\n",
    "    core = link[29:] # Ex: united-states/ca/san-francisco/2019-03-06/visualisations/reviews.csv\n",
    "    splitted = core.split(\"/\")\n",
    "    \n",
    "    return(splitted)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def get_metadata(splitted):\n",
    "    \n",
    "    if len(splitted) == 6: # Most links have 6 elements \n",
    "        country = splitted[0]\n",
    "        region = splitted[1]\n",
    "        city = splitted[2]\n",
    "        date = splitted[3]\n",
    "        filetype = splitted[4] # This is either 'data' or 'visualizations'; More like purpose...\n",
    "        filename = splitted[5] \n",
    "        filesavename = re.sub( '[^a-z0-9]', '', date) + \"_\" + city + \"_\" + filename # This is so we can name our files conveniently\n",
    "        return(city, date, filetype, filename, filesavename)\n",
    "\n",
    "    else: # This is for Ireland which has only 4 elements.\n",
    "        country = splitted[0]\n",
    "        # missing region\n",
    "        # missing city\n",
    "        date = splitted[1]\n",
    "        filetype = splitted[2]\n",
    "        filename = splitted[3]\n",
    "        filesavename = re.sub('[^a-z0-9]', '', date)  + \"_\" + country + \"_\" + filename # Example: 2019-11-23*ireland*listings.csv.gz\n",
    "        return(country, date, filetype, filename, filesavename)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def getall(my_list, s):\n",
    "    '''\n",
    "    Elegant list comprehension to match city name with index.\n",
    "    Matches second element of a tuple with the first one\n",
    "    '''\n",
    "    index = [x for x, y in my_list if y==s] # Very versatile. \n",
    "    return (index[0])\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def application_launch(option0_input=\"\", ):\n",
    "    '''The purpose is to be able to input the desired city that will be scraped.'''\n",
    "    # Input variables\n",
    "    \n",
    "    # USE NLP TO RETURN CLOSEST NEIGHBOUR. EX: YOU MEANT COPENHAGEN? or YOUR CITY OF INTEREST IS COPENHAGEN. IS THAT CORRECT?\n",
    "    \n",
    "    option0_input = str(input(\"One or many cities?. Answer with One/Many.\\n\")) # CAN ALSO DO LIST OF CITIES\n",
    "    \n",
    "    if option0_input == \"One\":\n",
    "        input_name = str(input(\"Please state the name of your desired city. Ex: Copenhagen.\")) \n",
    "        input_index =  getall(list_of_cities_index, input_name) # Link city name with index so we can search and find it in page source. \n",
    "        input_index_list = []\n",
    "        \n",
    "    elif option0_input == \"Many\":\n",
    "        many_string_input = str(input(\"Please enter a string representing a list of cities. Ex: Copenhagen,Berlin,Paris,etc.\\n\"))\n",
    "        many_list_input = list(many_string_input.split(\",\"))\n",
    "        input_index = \"\"\n",
    "        input_index_list = []\n",
    "        for c in many_list_input:\n",
    "            input_index_list.append(getall(list_of_cities_index,c))\n",
    "            \n",
    "    # Also ALL option?\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    option1_input = str(input(\"Do you want to download one or all files? Type One/All/Many.\\n\")) # CAN ALSO DO LIST OF CITIES\n",
    "    # IF NOT IN ANSWER LIST RETURN ERROR.\n",
    "    \n",
    "    if option1_input == \"One\":\n",
    "        direct_download_link_input = str(input(\"Please copy paste your direct download link here.\\n\"))\n",
    "        print(\"\\n\")\n",
    "        filetype_input = str(input(\"Please state the your desired file type to work on. Ex: 'calendar.csv.gz'.\\n\"))\n",
    "        \n",
    "    elif option1_input == \"All\":\n",
    "        direct_download_link_input = \"\"\n",
    "        filetype_input = str(input(\"Please state the your desired file type to work on. Ex: 'calendar.csv.gz'.\\n\"))\n",
    "        \n",
    "    #elif option1_input == \"Many\":\n",
    "     #   direct_download_link_input = \"\"\n",
    "      #  filetype_input = str(input(\"Please state the your desired file type to work on. Ex: 'calendar.csv.gz'.\\n\"))\n",
    "    else:\n",
    "        print(\"Error. Answer not acceptable. Please try again (have to implement this try again though.)\\n\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    option2_input = str(input(\"Do you want to specify the current working directory? Type Yes/No or Default..\\nDefault is for me. It runs the path I used when writing the code.\\n\"))\n",
    "    \n",
    "    if option2_input == \"Yes\": # USE REGEX TO IGNORE CASE SENSITIVITY\n",
    "        chdir_path_input = str(input(\"Type in or copy paste the path to which you want to store your output.\\nNote: I store it on an external HDD so it's possible.\\n\"))\n",
    "        # CURRENT DIR\n",
    "        os.chdir(r\"\" + str(chdir_path_input))\n",
    "        root_download_folder = str(os.getcwd())\n",
    "    \n",
    "    elif option2_input == \"No\": # Choosing no is your default machine cwd. \n",
    "        root_download_folder = str(os.getcwd())\n",
    "    \n",
    "    elif option2_input == \"Default\": \n",
    "        #default_chdir = r\"E:\\Airbnb\\Data\"\n",
    "        os.chdir()\n",
    "        root_download_folder = str(os.getcwd())\n",
    "    \n",
    "    else:\n",
    "        print(\"Error. Answer not acceptable. Please try again (have to implement this try again though.)\")\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return(input_index, direct_download_link_input, filetype_input, root_download_folder, input_index_list)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def write_zip_to_disk(path_to_zip_folder, path_to_zip_file, url):\n",
    "    \n",
    "    if not os.path.exists(path_to_zip_folder): # Need to create the path up to the file name. \n",
    "        os.makedirs(path_to_zip_folder)\n",
    "    with open(path_to_zip_file, \"wb\") as f:\n",
    "            r = requests.get(url)\n",
    "            f.write(r.content)\n",
    "            \n",
    "################################################################################################################\n",
    "\n",
    "def unzip(path_to_zip_file,path_to_unzipped_file):\n",
    "    with gzip.open(path_to_zip_file, 'rb') as f_in:\n",
    "        with open(path_to_unzipped_file, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "    os.remove(path_to_zip_file) # Delete the GZ after unzipping to free memory \n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def pre_process(df):\n",
    "    '''\n",
    "    Some level of standard pre-processing is required for these dataframes.\n",
    "    '''\n",
    "    # I think it's best if we create a copy of the calendar to play around with, for testing purposes\n",
    "    # In practice, we're wasting too much memory, so I'll just comment this out and set input to df\n",
    "    #df = calendar\n",
    "    \n",
    "    # But the price is an object because of the $ sign.\n",
    "    # Eliminating the $sign\n",
    "    df[\"price\"] = df[\"price\"].replace('[\\$,]', '', regex=True).astype(float)\n",
    "    # Then assigning new data type to numeric so we can do calculations.\n",
    "    df[\"price\"] = pd.to_numeric(df[\"price\"])\n",
    "    \n",
    "    # Also, the date needs to be changed to datetime. \n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    # We're only interested in price and time so we drop the rest.\n",
    "    # We ignore errors, as some calendars don't contain those columns \n",
    "    df.drop(columns=[\"available\",\"adjusted_price\",\"minimum_nights\",\"maximum_nights\",\"listing_id\"], inplace=True, errors='ignore')\n",
    "    \n",
    "    # I guess we should make this function return the \"new\" dataframe\n",
    "    return df\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def memory_optimization(df):\n",
    "    for i in ['year','quarter','month','day','weekday']:\n",
    "        df[i] = df[i].astype('category')       \n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def get_yearspan(df):\n",
    "    '''\n",
    "    A helper function that will return a list of the years present in the calendar file.\n",
    "    '''\n",
    "    years = df[\"date\"].dt.year.unique()\n",
    "    \n",
    "    return years \n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def split_by_year(df):\n",
    "    '''\n",
    "    Split a calendar dataframe into multiple dataframes, sliced by year value.\n",
    "    '''\n",
    "    \n",
    "    years = get_yearspan(df) # Get a list of the years; As far as I know, there can be at least 2 values\n",
    "    \n",
    "    \n",
    "    new_df_list = [] # A list that stockpiles the outputs \n",
    "    \n",
    "            \n",
    "    if len(years) >  1: # If there's more than 1 year, let's split that shit \n",
    "        for i in range(len(years)):\n",
    "            splice = df.loc[df['year'] == years[i],:]\n",
    "            new_df_list.append(splice)\n",
    "    \n",
    "    return new_df_list   \n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def statistics(df,column_name):\n",
    "    '''\n",
    "    A function that returns statistics such as mean, std, mode, etc.\n",
    "    The input is a dataframe and the desired column by which to group by.\n",
    "    Groups the results in dictionaries.\n",
    "    '''\n",
    "    \n",
    "    df = df.filter([column_name, 'price'], axis=1) # Filter to keep only price values. \n",
    "    \n",
    "    # Group by column.\n",
    "    mean = df.groupby([column_name]).mean()\n",
    "    \n",
    "    # Creating separate dataframes for the statistic\n",
    "    statistics = df.groupby([column_name]).describe()\n",
    "    \n",
    "    # Making the dataframe presentable\n",
    "    statistics = statistics.price.reset_index(level=[column_name])\n",
    "    \n",
    "    return statistics\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def create_mini_dataframes(df):\n",
    "    '''\n",
    "    This function creates the slices all at a time.\n",
    "    '''\n",
    "    \n",
    "    year = statistics(df,\"year\")\n",
    "    quarter = statistics(df,\"quarter\")\n",
    "    month = statistics(df,\"month\")\n",
    "    weekday = statistics(df,\"weekday\")\n",
    "    calendar_day = statistics(df,\"date\")\n",
    "    \n",
    "    # Returns a tuple with 5 elements.\n",
    "    return (year,quarter,month,weekday,calendar_day)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def apply_create_mini_dataframes(split_list):\n",
    "    '''\n",
    "    Argument is the output of create_mini_dataframes()\n",
    "    '''\n",
    "    \n",
    "    mini_collection_list = [] # For each year, tuples of the collections of statistics, for each 'timeframe'.\n",
    "\n",
    "    for i in range(len(split_list)):\n",
    "        # A collection of statistics specific to a certain year. Ex: 2020's stats for year, quarter, month, etc.\n",
    "        mini_collection = create_mini_dataframes(split_list[i])\n",
    "        mini_collection_list.append(mini_collection)\n",
    "        \n",
    "    return (mini_collection_list)\n",
    "            \n",
    "################################################################################################################\n",
    "\n",
    "def write_to_csv(path_to_mini_folder, mini_df):\n",
    "    \n",
    "    if not os.path.exists(path_to_mini_folder):\n",
    "        os.makedirs(path_to_mini_folder)\n",
    "        mini_df.to_csv(path_to_mini_folder +  \"\\\\\" + \"calendar.csv\", index=False)            \n",
    "            \n",
    "################################################################################################################\n",
    "\n",
    "def print_time_elapsed(start_time, end_time):\n",
    "    # The actual difference \n",
    "    time_elapsed = end_time - start_time\n",
    "    \n",
    "    if time_elapsed <=  60:\n",
    "        out =  \"{0:.2f}\".format(time_elapsed)\n",
    "        print(\"Time elapsed in seconds: \", out)\n",
    "        return(out)\n",
    "    \n",
    "    elif  (time_elapsed <= 3600) and (time_elapsed > 60) :\n",
    "        out = \"{0:.2f}\".format(time_elapsed/60)\n",
    "        print(\"Time elapsed in minutes: \", out)\n",
    "        return(out)\n",
    "    \n",
    "    else:\n",
    "        out = \"{0:.2f}\".format(time_elapsed/3600)\n",
    "        print(\"Time elapsed in hours: \", out)\n",
    "        return(out)\n",
    "         \n",
    "################################################################################################################\n",
    "\n",
    "def create_paths(city, filetype, root_download_folder, filename, filesavename):\n",
    "    \n",
    "    if filetype ==  \"data\":\n",
    "        path_to_download_folder = root_download_folder + \"\\\\\" + city + \"\\\\\"  + filename[:-7]\n",
    "        path_to_zip_file = path_to_download_folder + \"\\\\\" + filesavename\n",
    "        path_to_unzipped_file = path_to_download_folder + \"\\\\\" + filesavename[:-3] # Only GZ files need this additional path\n",
    "        \n",
    "    elif filetype == \"visualisations\":\n",
    "        \n",
    "        if filename != \"neighbourhoods.geojson\":\n",
    "            path_to_download_folder = root_download_folder + \"\\\\\" + city + \"\\\\\"  + filename[:-4]\n",
    "            path_to_zip_file = path_to_download_folder + \"\\\\\" + filesavename\n",
    "            path_to_unzipped_file = \"\"\n",
    "\n",
    "        else:\n",
    "            path_to_download_folder = root_download_folder + \"\\\\\" + city + \"\\\\\"  + filename[:-8]\n",
    "            path_to_zip_file = path_to_download_folder + \"\\\\\" + filesavename\n",
    "            path_to_unzipped_file = \"\"\n",
    "        \n",
    "    return (path_to_download_folder, path_to_zip_file, path_to_unzipped_file)\n",
    "    \n",
    "################################################################################################################\n",
    "\n",
    "def dataframe_work(path_to_unzipped_file, path_to_download_folder, date):\n",
    "    \n",
    "    # Create a Data Frame\n",
    "    df = pd.read_csv(path_to_unzipped_file)\n",
    "        \n",
    "    # Pre-process dataframe \n",
    "    df = pre_process(df) # The data is in a very specific structured format.'''\n",
    "        \n",
    "    # Optimize memory allocation \n",
    "    #memory_optimization(df) #Makes the files considerably smaller in size.\n",
    "        \n",
    "    # Year list\n",
    "    #years = get_yearspan(df) #Just a helper.\n",
    "        \n",
    "    # Splitting by year\n",
    "    #df_list = split_by_year(df) #Just a helper.'''\n",
    "        \n",
    "    # Create the mini files\n",
    "    mini_df = statistics(df, \"date\")\n",
    "        \n",
    "    # Path to storage folder\n",
    "    path_to_mini_folder = path_to_download_folder + \"\\\\minis\\\\\" +  date\n",
    "        \n",
    "    # Write them to CSV\n",
    "    write_to_csv(path_to_mini_folder, mini_df) # The output.'''\n",
    "        \n",
    "    # Delete main file\n",
    "    os.remove(path_to_unzipped_file) # Of course we want to get rid of this almost .5GB monster.'''\n",
    "    \n",
    "################################################################################################################\n",
    "\n",
    "def scrape_one_market(direct_download_link_input, filetype_input, market, root_download_folder):\n",
    "    \n",
    "    if direct_download_link_input:\n",
    "        \n",
    "        link = direct_download_link_input\n",
    "        splitted = link_core(link)\n",
    "\n",
    "        metadata = get_metadata(splitted)\n",
    "\n",
    "        city = metadata[0]\n",
    "        date = metadata[1]\n",
    "        filetype = metadata[2]\n",
    "        filename = metadata[3]\n",
    "        filesavename = metadata[4]\n",
    "    \n",
    "        if (filetype_input == \"calendar.csv.gz\") and (filename == filetype_input):\n",
    "\n",
    "            paths = create_paths(city, filetype, root_download_folder, filename, filesavename)\n",
    "\n",
    "            path_to_download_folder = paths[0]\n",
    "            path_to_zip_file = paths[1]\n",
    "            path_to_unzipped_file = paths[2]\n",
    "\n",
    "            write_zip_to_disk(path_to_download_folder, path_to_zip_file, link)\n",
    "            unzip(path_to_zip_file, path_to_unzipped_file)\n",
    "\n",
    "            dataframe_work(path_to_unzipped_file, path_to_download_folder, date)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        for a in market.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "            \n",
    "            # Find all links \n",
    "            link = a.get('href')\n",
    "            splitted = link_core(link)\n",
    "\n",
    "            metadata = get_metadata(splitted)\n",
    "\n",
    "            city = metadata[0]\n",
    "            date = metadata[1]\n",
    "            filetype = metadata[2]\n",
    "            filename = metadata[3]\n",
    "            filesavename = metadata[4]\n",
    "        \n",
    "            if (filetype_input == \"calendar.csv.gz\") and (filename == filetype_input):\n",
    "\n",
    "                paths = create_paths(city, filetype, root_download_folder, filename, filesavename)\n",
    "\n",
    "                path_to_download_folder = paths[0]\n",
    "                path_to_zip_file = paths[1]\n",
    "                path_to_unzipped_file = paths[2]\n",
    "\n",
    "                write_zip_to_disk(path_to_download_folder, path_to_zip_file, link)\n",
    "                unzip(path_to_zip_file, path_to_unzipped_file)\n",
    "\n",
    "                dataframe_work(path_to_unzipped_file, path_to_download_folder, date)\n",
    "                           \n",
    "################################################################################################################\n",
    "       \n",
    "def my_excepthook(type, value, traceback):\n",
    "    end = time.time()\n",
    "    print(\"Program crashed after\", end - start, \"seconds\")\n",
    "    sys.__excepthook__(type, value, traceback)  # Print error message\n",
    "    \n",
    "################################################################################################################\n",
    "\n",
    "def send_sms(duration):\n",
    "    # the following line needs your Twilio Account SID and Auth Token\n",
    "    client = Client(\"AC5e04cc29173e103d25fe0bacaa3bcf4a\", \"84d11e73db927ce27813f6c5d7bdd926\")\n",
    "    \n",
    "    content = \"Work finished in \" + duration\n",
    "    \n",
    "    # change the \"from_\" number to your Twilio number and the \"to\" number\n",
    "    # to the phone number you signed up for Twilio with, or upgrade your\n",
    "    # account to send SMS to any phone number\n",
    "    client.messages.create(to=\"+4571530372\", \n",
    "                           from_=\"+12055761457\", \n",
    "                           body=content)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def show_all_show_more(driver):\n",
    "    clicks = 0\n",
    "    while True:\n",
    "        clicks += 1\n",
    "        # 98 clicks because...idk man, not all have show more data\n",
    "        if clicks <= 98: # Here we tell it how many times to click before it stops. \n",
    "            driver.find_element(By.PARTIAL_LINK_TEXT, \"show\").click()\n",
    "            #sleep(randint(3,12))\n",
    "        else: \n",
    "            break\n",
    "            \n",
    "################################################################################################################\n",
    "\n",
    "def show_many_show_more(driver, input_index_list):\n",
    "    for index in input_index_list: \n",
    "        driver.find_elements(By.PARTIAL_LINK_TEXT,\"show\")[index].click() \n",
    "\n",
    "################################################################################################################\n",
    "            \n",
    "def show_more(driver, input_index):\n",
    "    driver.find_elements(By.PARTIAL_LINK_TEXT,\"show\")[input_index].click() \n",
    "    \n",
    "################################################################################################################\n",
    "\n",
    "def scrape_many_markets(filetype_input, market_list, root_download_folder):\n",
    "    \n",
    "    for market in market_list:\n",
    "        \n",
    "        for a in market.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "            \n",
    "            # Find all links \n",
    "            link = a.get('href')\n",
    "            splitted = link_core(link)\n",
    "\n",
    "            metadata = get_metadata(splitted)\n",
    "\n",
    "            city = metadata[0]\n",
    "            date = metadata[1]\n",
    "            filetype = metadata[2]\n",
    "            filename = metadata[3]\n",
    "            filesavename = metadata[4]\n",
    "        \n",
    "            if (filetype_input == \"calendar.csv.gz\") and (filename == filetype_input):\n",
    "\n",
    "                paths = create_paths(city, filetype, root_download_folder, filename, filesavename)\n",
    "\n",
    "                path_to_download_folder = paths[0]\n",
    "                path_to_zip_file = paths[1]\n",
    "                path_to_unzipped_file = paths[2]\n",
    "\n",
    "                write_zip_to_disk(path_to_download_folder, path_to_zip_file, link)\n",
    "                unzip(path_to_zip_file, path_to_unzipped_file)\n",
    "\n",
    "                dataframe_work(path_to_unzipped_file, path_to_download_folder, date)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'Amsterdam'), (1, 'Antwerp'), (2, 'Asheville'), (3, 'Athens'), (4, 'Austin'), (5, 'Barcelona'), (6, 'Barossa Valley'), (7, 'Barwon South West'), (8, 'Beijing'), (9, 'Belize'), (10, 'Bergamo'), (11, 'Berlin'), (12, 'Bologna'), (13, 'Bordeaux'), (14, 'Boston'), (15, 'Bristol'), (16, 'Broward County'), (17, 'Brussels'), (18, 'Buenos Aires'), (19, 'Cambridge'), (20, 'Cape Town'), (21, 'Chicago'), (22, 'Clark County'), (23, 'Columbus'), (24, 'Copenhagen'), (25, 'Crete'), (26, 'Denver'), (27, 'Dublin'), (28, 'Edinburgh'), (29, 'Euskadi'), (30, 'Florence'), (31, 'Geneva'), (32, 'Ghent'), (33, 'Girona'), (34, 'Greater Manchester'), (35, 'Hawaii'), (36, 'Hong Kong'), (37, 'Istanbul'), (38, 'Jersey City'), (39, 'Lisbon'), (40, 'London'), (41, 'Los Angeles'), (42, 'Lyon'), (43, 'Madrid'), (44, 'Malaga'), (45, 'Mallorca'), (46, 'Manchester'), (47, 'Melbourne'), (48, 'Menorca'), (49, 'Mexico City'), (50, 'Milan'), (51, 'Montreal'), (52, 'Munich'), (53, 'Naples'), (54, 'Nashville'), (55, 'New Brunswick'), (56, 'New Orleans'), (57, 'New York City'), (58, 'Northern Rivers'), (59, 'Oakland'), (60, 'Oslo'), (61, 'Ottawa'), (62, 'Pacific Grove'), (63, 'Paris'), (64, 'Portland'), (65, 'Porto'), (66, 'Prague'), (67, 'Puglia'), (68, 'Quebec City'), (69, 'Rhode Island'), (70, 'Rio de Janeiro'), (71, 'Rome'), (72, 'Salem'), (73, 'San Diego'), (74, 'San Francisco'), (75, 'San Mateo County'), (76, 'Santa Clara County'), (77, 'Santa Cruz County'), (78, 'Santiago'), (79, 'Seattle'), (80, 'Sevilla'), (81, 'Sicily'), (82, 'Singapore'), (83, 'South Aegean'), (84, 'Stockholm'), (85, 'Sydney'), (86, 'Taipei'), (87, 'Tasmania'), (88, 'Thessaloniki'), (89, 'Tokyo'), (90, 'Toronto'), (91, 'Trentino'), (92, 'Twin Cities MSA'), (93, 'Valencia'), (94, 'Vancouver'), (95, 'Vaud'), (96, 'Venice'), (97, 'Victoria'), (98, 'Vienna'), (99, 'Washington'), (100, 'Western Australia'), (101, 'Zurich')]\n",
      "\n",
      "\n",
      "\n",
      "One or many cities?. Answer with One/Many.\n",
      "Many\n",
      "Please enter a string representing a list of cities. Ex: Copenhagen,Berlin,Paris,etc.\n",
      "Paris\n",
      "\n",
      "\n",
      "Do you want to download one or all files? Type One/All/Many.\n",
      "All\n",
      "Please state the your desired file type to work on. Ex: 'calendar.csv.gz'.\n",
      "calendar.csv.gz\n",
      "\n",
      "\n",
      "Do you want to specify the current working directory? Type Yes/No or Default..\n",
      "Default is for me. It runs the path I used when writing the code.\n",
      "Yes\n",
      "Type in or copy paste the path to which you want to store your output.\n",
      "Note: I store it on an external HDD so it's possible.\n",
      "C:\\Users\\aleen\\Desktop\\Master Thesis\\Data\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################################################\n",
    "\n",
    "# Excepthook\n",
    "sys.excepthook = my_excepthook\n",
    "\n",
    "# Source code \n",
    "source_code = try_connecting(get_the_data_url)\n",
    "\n",
    "# Getting source code \n",
    "plain_text = source_code.text\n",
    "\n",
    "# soup #1\n",
    "soup = BeautifulSoup(plain_text, 'html.parser')\n",
    "\n",
    "# City name list \n",
    "cities_list = make_cities_list(soup)\n",
    "\n",
    "# No of cities\n",
    "#no_cities = len(cities_list)\n",
    "\n",
    "# Index list\n",
    "list_of_cities_index = make_cities_index_list(cities_list)\n",
    "\n",
    "print(list_of_cities_index)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Launch phase\n",
    "launch = application_launch()\n",
    "\n",
    "# Input\n",
    "input_index = launch[0]\n",
    "direct_download_link_input = launch[1]\n",
    "filetype_input = launch[2]\n",
    "root_download_folder = launch[3]\n",
    "input_index_list = launch[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed in seconds:  25.56\n"
     ]
    }
   ],
   "source": [
    "################################################################################################################\n",
    "\n",
    "# Runtime starting time\n",
    "start_time = time.time()\n",
    "\n",
    "# Selenium driver\n",
    "driver = webdriver.Firefox() # Firefox Driver\n",
    "driver.get(get_the_data_url)\n",
    "\n",
    "show_many_show_more(driver, input_index_list)\n",
    "    \n",
    "# New source code from driver\n",
    "source_code = driver.page_source\n",
    "soup = BeautifulSoup(source_code, 'html.parser')\n",
    "\n",
    "market_list = []\n",
    "for index in input_index_list:\n",
    "    market_list.append(soup.findAll('table')[index])\n",
    "                                \n",
    "# Runtime ending time\n",
    "end_time = time.time()\n",
    "duration = print_time_elapsed(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not a gzipped file (b'<h')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-10950f0daab4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# SCRAPER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mscrape_many_markets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiletype_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiletype_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarket_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmarket_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot_download_folder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mroot_download_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Runtime ending time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-48caab9f0b12>\u001b[0m in \u001b[0;36mscrape_many_markets\u001b[1;34m(filetype_input, market_list, root_download_folder)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m                 \u001b[0mwrite_zip_to_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_download_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_zip_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m                 \u001b[0munzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_zip_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_unzipped_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m                 \u001b[0mdataframe_work\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_unzipped_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_download_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-48caab9f0b12>\u001b[0m in \u001b[0;36munzip\u001b[1;34m(path_to_zip_file, path_to_unzipped_file)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_zip_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_unzipped_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_out\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_zip_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Delete the GZ after unzipping to free memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;34m\"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    461\u001b[0m                 \u001b[1;31m# jump to the next member, if there is one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_gzip_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36m_read_gzip_header\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34mb'\\037\\213'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Not a gzipped file (%r)'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmagic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         (method, flag,\n",
      "\u001b[1;31mOSError\u001b[0m: Not a gzipped file (b'<h')"
     ]
    }
   ],
   "source": [
    "################################################################################################################\n",
    "\n",
    "# Runtime starting time\n",
    "start_time = time.time()\n",
    "\n",
    "# SCRAPER\n",
    "scrape_many_markets(filetype_input=filetype_input, market_list=market_list, root_download_folder=root_download_folder)\n",
    "\n",
    "# Runtime ending time\n",
    "end_time = time.time()\n",
    "duration = print_time_elapsed(start_time, end_time)\n",
    "\n",
    "# Send SMS\n",
    "#send_sms(duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
