{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Working with dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Maths; basic library\n",
    "import numpy as np\n",
    "\n",
    "# Let's just ignore ignore warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# Time keeps us keep track of how fast our script is \n",
    "import time\n",
    "\n",
    "# This is good for working with system files and folders \n",
    "import os \n",
    "\n",
    "# For doing out-of-the-box sentiment analysis \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer() # Creating a reference to the object for easy use\n",
    "\n",
    "# we use Python's langdetect to detect languages present in the comments \n",
    "from langdetect import detect\n",
    "\n",
    "# Regex is always handy when working with text data \n",
    "import re\n",
    "\n",
    "# API from Google for translating sentences to English, so we can use them in Vader\n",
    "from googletrans import Translator\n",
    "translator = Translator() # Creating a reference to the object for easy use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "def fill_nans(df,feature,filling=\"mean\"):\n",
    "    '''\n",
    "    Fills Nans by replacing them with either median, mean, or a value chosen by the user.\n",
    "    Default filling value is mean.\n",
    "    '''\n",
    "    if filling == \"median\":\n",
    "        median = df[feature].median()\n",
    "        df[feature] = df[feature].fillna(median)\n",
    "    elif filling == \"mean\":\n",
    "        mean = df[feature].mean()\n",
    "        df[feature] = df[feature].fillna(mean)\n",
    "    else:\n",
    "        df[feature].fillna(filling, inplace=True)\n",
    "        \n",
    "def language_detection(text):\n",
    "    '''\n",
    "    Function that tries to detect the language of a comment.\n",
    "    '''\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def print_sentiment_scores(sentence):\n",
    "    '''\n",
    "    Prints all of the sentiment scores of a given text.\n",
    "    '''\n",
    "    snt = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(snt)))\n",
    "\n",
    "def sentiment_score(text, sentiment):\n",
    "    '''\n",
    "    Returns the sentiment value. Input is the text and desired sentiment: neg, neu, pos and compound.\n",
    "    '''\n",
    "    if sentiment == \"neg\":\n",
    "        negative_value = analyzer.polarity_scores(text)['neg']\n",
    "        return negative_value\n",
    "    elif sentiment == \"neu\":\n",
    "        neutral_value = analyzer.polarity_scores(text)['neu']\n",
    "        return neutral_value\n",
    "    elif sentiment == \"pos\":\n",
    "        positive_value = analyzer.polarity_scores(text)['pos']\n",
    "        return positive_value\n",
    "    elif sentiment == \"compound\":\n",
    "        compound_value = analyzer.polarity_scores(text)['compound']\n",
    "        return compound_value\n",
    "    else: \n",
    "        print(\"This is not a valid sentiment. Please refer to the function docstring.\")\n",
    "        \n",
    "def translator_compound_scores(text, engl=True):\n",
    "    '''\n",
    "    This function uses the Google Translate API to check if a text is in English.\n",
    "    If not, it translates it. Then, Vader calculates its compound score. \n",
    "    Then, the function assigns either 1 (positive), 0 (neutral) or -1(negative) based on rules.\n",
    "    '''\n",
    "    # Checks language\n",
    "    if engl:\n",
    "        trans = text\n",
    "    else: # If not English, translate it.\n",
    "        trans = translator.translate(text).text\n",
    "    \n",
    "    # Going to be using Sentiment Analysys \n",
    "    score = analyzer.polarity_scores(trans)\n",
    "    lb = score['compound'] # Compount polarity score is what interests us. \n",
    "    \n",
    "    # Rules for output\n",
    "    if lb >= 0.05:\n",
    "        return 1\n",
    "    elif (lb > -0.05) and (lb < 0.05):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def clean(doc):\n",
    "    '''\n",
    "    Function that does some basic text cleaning. \n",
    "    '''\n",
    "    stop_free = \" \".join([word for word in doc.lower().split() if word not in stop])\n",
    "    punc_free = \"\".join(token for token in stop_free if token not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "def get_sentiment(reviews_path,listings_path):\n",
    "    '''\n",
    "    Function that returns the average sentiment score of a listing. The output is a dataframe with the listing ID \n",
    "    and the sentiment score (arithmetical mean).\n",
    "    '''\n",
    "    # Start time to keep a note of how long it took\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Loading Data\n",
    "    reviews = pd.read_csv(reviews_path)\n",
    "    listings = pd.read_csv(listings_path)\n",
    "    \n",
    "    # Dropping unneccesary columns from the reviews dataset.\n",
    "    reviews.drop([\"date\",\"reviewer_id\",\"reviewer_name\",\"id\"], axis=1,inplace=True)\n",
    "\n",
    "    # Merging the dataframes, only adding specific columns. \n",
    "    df = pd.merge(reviews, listings, left_on='listing_id', right_on='id', how='left')\n",
    "    \n",
    "    # Filling nans \n",
    "    fill_nans(df,\"number_of_reviews\",0)\n",
    "    fill_nans(df,\"summary_text\",\"\")\n",
    "    \n",
    "    # Where there no comments present, just replace with \"\", an empty string.\n",
    "    fill_nans(df,\"comments\",\"\")\n",
    "    \n",
    "    # renaming columns \n",
    "    df.drop([\"id\"], axis=1,inplace=True)\n",
    "    \n",
    "    # THE ACTUAL WORK TAKING PLACE\n",
    "    df['reviews_sentiment'] = df['comments'].apply(translator_compound_scores)\n",
    "    df['summary_sentiment'] = df['summary_text'].apply(translator_compound_scores)\n",
    "    \n",
    "    # This is the output \n",
    "    mean_sentiment = df.groupby('listing_id').mean()\n",
    "    \n",
    "    # End time \n",
    "    end_time = time.time()\n",
    "    print(\"Language detection, translation and sentiment analysis was succesfully finished in \",end_time-start_time,\" seconds.\")\n",
    "    \n",
    "    mean_sentiment = mean_sentiment.dropna()\n",
    "    return(mean_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r\"C:\\Users\\aleen\\Desktop\\Reviews\\paris\"\n",
    "reviews = pd.read_csv(r\"C:\\Users\\aleen\\Desktop\\Reviews\\paris\\paris_reviews.csv\")\n",
    "listings = pd.read_csv(r\"C:\\Users\\aleen\\Desktop\\Reviews\\paris\\paris_listings_root.csv\")\n",
    "reviews_path = r\"C:\\Users\\aleen\\Desktop\\Reviews\\paris\\paris_reviews.csv\"\n",
    "listings_path = r\"C:\\Users\\aleen\\Desktop\\Reviews\\paris\\paris_listings_root.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language detection, translation and sentiment analysis was succesfully finished in  1037.6510257720947  seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25474, 80)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_sentiment = get_sentiment(reviews_path, listings_path)\n",
    "mean_sentiment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sentiment.to_csv(r\"C:\\Users\\aleen\\Desktop\\Reviews\\paris\\paris_listings_plus_sentiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
